# -*- coding: utf-8 -*-
"""dsci_final_project_30k_models4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KtT4cpqs7bgwz9KKyQpQqmKPHcIVAyQ1
"""

#!pip install --upgrade tensorflow

#pip install datasets

#from google.colab import drive
#drive.mount('/content/drive')

import tensorflow as tf
from tensorflow import keras
print("tensorflow version:", tf.__version__)
print("keras version:", keras.__version__)

from datasets import load_dataset

dataset = load_dataset("nlphuji/flickr30k", streaming=True)

"""# Explore Dataset"""

type(dataset)

print(dataset)

# Access the dataset
train_dataset = dataset['test']

# Print number of total images in dataset
print("Number of total images in dataset:", sum(1 for _ in train_dataset))

# Display the first example in the test split
#print(test_dataset[0])

# Display the first 5 examples in the test split
#for i in range(5):
 #   print(test_dataset[i])
for example in train_dataset:
    print(example)
    break

# Access the first image and its corresponding caption
#first_example = test_dataset[0]
for example in train_dataset:
    first_example=example
    break
image = first_example['image']
caption = first_example['caption']
print(f"Image: {image}")
print(f"Caption: {caption}")

# Display other features
sentids = first_example['sentids']
split = first_example['split']
img_id = first_example['img_id']
filename = first_example['filename']
print(f"Sent IDs: {sentids}")
print(f"Split: {split}")
print(f"Image ID: {img_id}")
print(f"Filename: {filename}")

from PIL import Image
from IPython.display import display

# Display the first image and its caption
image = first_example['image']
caption = first_example['caption']

# Display the image inline
display(image)
print(f"Caption: {caption}")

"""# Perform Data Cleaning"""

# Function to create a description dictionary that will map images with all 5 captions
def img_capt(dataset_split):
    descriptions = {}
    for example in dataset_split:
        filename = example['filename']
        captions = example['caption']
        descriptions[filename] = captions
    return descriptions

# Pass the correct split to the function
descriptions = img_capt(train_dataset)

print(len(descriptions))
print(next(iter(descriptions.items())))
print(type(descriptions))

# Print a few entries to verify
for key, captions in list(descriptions.items())[:5]:
    print((key, captions))

# Function to convert all upper case alphabets to lowercase, removing punctuations and words containing numbers
import string
import re

def txt_cleaning(descriptions):
    table = str.maketrans('', '', string.punctuation)
    cleaned_descriptions = {}

    for key, captions in descriptions.items():
        cleaned_captions = []
        for caption in captions:
            # Convert to lowercase
            caption = caption.lower()
            # Remove punctuation
            caption = caption.translate(table)
            # Remove words containing numbers
            caption = re.sub(r'\w*\d\w*', '', caption)
            # Remove extra spaces
            caption = ' '.join(caption.split())
            cleaned_captions.append(caption)
        cleaned_descriptions[key] = cleaned_captions

    return cleaned_descriptions

cleaned_descriptions = txt_cleaning(descriptions)

print(len(cleaned_descriptions))
print(type(cleaned_descriptions))
print(next(iter(cleaned_descriptions.items())))

# Function to create a vocabulary
def txt_vocab(descriptions):
    vocabulary = set()
    for key in descriptions.keys():
        for caption in descriptions[key]:
            vocabulary.update(caption.split())
    return vocabulary

vocabulary = txt_vocab(cleaned_descriptions)

print(len(vocabulary))
print(list(vocabulary)[:10])

"""# Extracting the feature vector (do beforehand on google colab)"""
'''
# Function to load and resize the images to 299x299
from keras.preprocessing.image import load_img, img_to_array
from keras.applications.xception import Xception, preprocess_input
import numpy as np

def preprocess_image(image, target_size=(299, 299)):
    image = image.resize(target_size)
    image = img_to_array(image)
    image = np.expand_dims(image, axis=0)
    image = preprocess_input(image)
    return image

# Load the Pre-trained Xception Model
from keras.applications.xception import Xception

# Load the Xception model without the top classification layer
model = Xception(include_top=False, pooling='avg')

# Function to extract feature vectors
def extract_features(image, model):
    # Preprocess the image
    preprocessed_image = preprocess_image(image)
    # Get the feature vector
    features = model.predict(preprocessed_image)
    return features

# Extract features for all images in the dataset
features_dict = {}

for example in train_dataset:
    image = example['image']
    filename = example['filename']
    features = extract_features(image, model)
    features_dict[filename] = features
'''
"""Started cell above on Thu 6/6/2024 at around 7:48 PM. <br/>
Finished running on Fri 6/7/2024 at around 1:42 AM. <br/>
The cell above took around 6 hours to run.
"""

'''
# Save features to a file
import pickle

with open('/content/drive/My Drive/Colab Notebooks/dsci471/models4/features_dict_30k_4.pkl', 'wb') as f:
    pickle.dump(features_dict, f)
'''

"""# Loading dataset for Training the model"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical, plot_model
from keras.applications.xception import Xception, preprocess_input
from keras.layers import Input, Dense, Dropout, Embedding, LSTM, add
from keras.models import Model
from keras.preprocessing.image import img_to_array
from PIL import Image
import numpy as np
import string, re, pickle
from sklearn.model_selection import train_test_split
from tqdm import tqdm

# Shuffle and Split the Dataset Indexes
from sklearn.model_selection import train_test_split

num_examples = sum(1 for _ in train_dataset)
indexes = np.arange(num_examples)
train_indexes, test_indexes = train_test_split(indexes, test_size=0.2, random_state=42)
train_indexes = [int(idx) for idx in train_indexes]
test_indexes = [int(idx) for idx in test_indexes]

print(f"Training set size: {len(train_indexes)}")
print(f"Test set size: {len(test_indexes)}")

# Get image names
def get_image_names(dataset, indexes):
    # return [dataset[i]['filename'] for i in indexes]
    # Convert indexes to a set for fast lookup
    indexes_set = set(indexes)
    image_names = []
    # Iterate through the dataset
    for idx, item in enumerate(dataset):
        if idx in indexes_set:
            image_names.append(item['filename'])
        # Break early if we have collected all required indexes
        if len(image_names) == len(indexes_set):
            break
    return image_names

train_image_names = get_image_names(train_dataset, train_indexes)
test_image_names = get_image_names(train_dataset, test_indexes)

'''
with open('/content/drive/My Drive/Colab Notebooks/dsci471/models4/train_image_names.txt', 'w') as file:
    for train_image_name in train_image_names:
        file.write(train_image_name + '\n')

with open('/content/drive/My Drive/Colab Notebooks/dsci471/models4/test_image_names.txt', 'w') as file:
    for test_image_name in test_image_names:
        file.write(test_image_name + '\n')
'''
with open('train_image_names.txt', 'w') as file:
    for train_image_name in train_image_names:
        file.write(train_image_name + '\n')

with open('test_image_names.txt', 'w') as file:
    for test_image_name in test_image_names:
        file.write(test_image_name + '\n')

# Define the function to load cleaned descriptions with <start> and <end> tokens
def load_clean_descriptions(cleaned_descriptions, photos):
    descriptions = {}
    for image_id in photos:
        if image_id in cleaned_descriptions:
            descriptions[image_id] = []
            for caption in cleaned_descriptions[image_id]:
                desc = '<start> ' + caption + ' <end>'
                descriptions[image_id].append(desc)
    return descriptions

# Load features for training images
def load_features(photos):
    with open("features_dict_30k_4.pkl", "rb") as f:
        all_features = pickle.load(f)
    features = {k: all_features[k] for k in photos}
    return features

train_descriptions = load_clean_descriptions(cleaned_descriptions, train_image_names)
train_features = load_features(train_image_names)

print(f"Number of training descriptions: {len(train_descriptions)}")
print(f"Number of training features: {len(train_features)}")
print(f"Number of train images: {len(train_image_names)}")
print(f"Number of test images: {len(test_image_names)}")

example_img_name = train_image_names[0]
print(f"Feature vector for image {example_img_name}: {train_features[example_img_name]}")
print(f"Captions for image {example_img_name}: {train_descriptions[example_img_name]}")

# Function to get the image by filename
def get_image_by_filename(dataset, filename):
    for example in dataset:
        if example['filename'] == filename:
            return example['image']
    return None

# Get the image
get_image = get_image_by_filename(train_dataset, example_img_name)

# Display the image inline
display(get_image)

"""# Tokenizing the vocabulary"""

#converting dictionary to clean list of descriptions
def dict_to_list(descriptions):
    all_desc = []
    for key in descriptions.keys():
        [all_desc.append(d) for d in descriptions[key]]
    return all_desc

#from keras.preprocessing.text import Tokenizer
def create_tokenizer(descriptions):
    desc_list = dict_to_list(descriptions)
    tokenizer = Tokenizer()
    tokenizer.fit_on_texts(desc_list)
    return tokenizer

# give each word an index, and store that into tokenizer pickle file
tokenizer = create_tokenizer(train_descriptions)

with open('tokenizer_4_tux.pkl', 'wb') as f:
    pickle.dump(tokenizer, f)

#dump(tokenizer, open('tokenizer.p', 'wb'))
vocab_size = len(tokenizer.word_index) + 1
vocab_size
print("vocab_size:", vocab_size)

#calculate maximum length of descriptions
def max_length(descriptions):
    desc_list = dict_to_list(descriptions)
    return max(len(d.split()) for d in desc_list)

max_length = max_length(descriptions)
max_length
print("max_length:", max_length)

"""# Create Data generator"""

def data_generator(captions, features, tokenizer, max_length):
    while True:
        for key, captions_list in captions.items():
            feature = features[key][0]
            inp_image, inp_seq, op_word = create_sequences(tokenizer, max_length, captions_list, feature)
            yield (inp_image, inp_seq), op_word

def create_sequences(tokenizer, max_length, captions_list, feature):
    x_1, x_2, y = list(), list(), list()
    for caption in captions_list:
        seq = tokenizer.texts_to_sequences([caption])[0]
        for i in range(1, len(seq)):
            in_seq, out_seq = seq[:i], seq[i]
            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]
            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]
            x_1.append(feature)
            x_2.append(in_seq)
            y.append(out_seq)
    return np.array(x_1), np.array(x_2), np.array(y)

# Check the shape of the input and output for the model
[a,b],c = next(data_generator(train_descriptions, train_features, tokenizer, max_length))
a.shape, b.shape, c.shape
print("a.shape:", a.shape)
print("b.shape:", b.shape)
print("c.shape:", c.shape)

"""# Defining the CNN-RNN model"""

from keras.layers import add
from keras.models import Model, load_model
from keras.layers import Input, Dense#Keras to build our CNN and LSTM
from keras.layers import LSTM, Embedding, Dropout
from keras.utils import plot_model

def define_model(vocab_size, max_length):
    # features from the CNN model compressed from 2048 to 256 nodes
    inputs1 = Input(shape=(2048,))
    fe1 = Dropout(0.5)(inputs1)
    fe2 = Dense(256, activation='relu')(fe1)

    # LSTM sequence model
    inputs2 = Input(shape=(max_length,))
    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)
    se2 = Dropout(0.5)(se1)
    se3 = LSTM(256)(se2)

    # Merging both models
    decoder1 = add([fe2, se3])
    decoder2 = Dense(256, activation='relu')(decoder1)
    outputs = Dense(vocab_size, activation='softmax')(decoder2)

    # merge it [image, seq] [word]
    model = Model(inputs=[inputs1, inputs2], outputs=outputs)
    model.compile(loss='categorical_crossentropy', optimizer='adam')

    # summarize model
    print(model.summary())
    plot_model(model, to_file='model.png', show_shapes=True)

    return model

"""# Training the model"""

import os

# train our model
print('Dataset Total Images:', num_examples)
print('Descriptions: train =', len(train_descriptions))
print('Photos: train =', len(train_features))
print('Number of train images:', len(train_image_names))
print('Number of test images:', len(test_image_names))
print('Vocabulary Size:', vocab_size)
print('Description Length: ', max_length)

model = define_model(vocab_size, max_length)

epochs = 10
steps = len(train_descriptions)

for i in range(epochs):
    generator = data_generator(train_descriptions, train_features, tokenizer, max_length)
    model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)
    if i == 0 or i == 4 or i == 9:
      model.save("models/model_30k_" + str(i) + ".keras")
